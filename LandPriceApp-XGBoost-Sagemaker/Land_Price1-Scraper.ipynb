{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAND PRICE PREDICTION APP USING AWS SAGEMAKER'S IN-BUILT XGBOOST  - End-to-End\n",
    "We will build a Land Price Prediction App to help people looking to buy land in Cameroon, get the expected price of land per quartier they intend to buy land from.\n",
    "The following steps will be taken:\n",
    "- I)   PROBLEM STATEMENT:\n",
    "\n",
    "Many people in Cameroon want to buy lands and they have trouble getting information on what to expect as price per square metre for the quartier they want to buy the land from.They also want to be able to consult the prices of several quartiers before making their final choice.\n",
    "This is a difficult process in Cameroon as it will mean these people who want to buy lands will have to go about making many phone calls to people asking them the price of land in those quartiers.\n",
    "So the objective is to scrape the data already available on the biggest Classified adds website in Cameroon (Jumia Cameroon) https://www.jumia.cm/en/land-plots\n",
    "\n",
    "This data will be cleaned and trained using the in-built XGBoost Algorithm on AWS Sagemaker, and an endpoint will be created in AWS ,which wll be used to make predictions when given the inputs like \n",
    "- The Quartier the customer wants to buy land from\n",
    "- The size of the land the customer intends to buy (in metres square)\n",
    "- And the output of the model will be the predicted Price per metres square for the Quartier the customer requested.\n",
    "\n",
    "\n",
    "- II)   SCRAPING THE DATA:\n",
    "\n",
    "Scrape the data from a Classified ads website, where people post lands for sale per quartier in Cameroon.They typically type in the price per metres square and the total area of the land availlable for sale.\n",
    "- III)  PERFORM EXPLORATORY DATA ANALYSIS \n",
    "\n",
    "Inspect the data to validate the quality of the data scraped from the classified ads website. Analyse the distribution of missing values, outliers and gain other relevant insights from the model\n",
    "- IV) DO FEATURE ENGINEERING & SELECTION\n",
    "\n",
    "Handle the mising values, outliers and do the necessary transformations which will ensure the data is well suited for the machine learning model.And also to maximise the insights gotten from the Exploratory Data Analysis phase.\n",
    "- V)  BUILD,TRAIN AND DEPLOY THE MODEL IN SAGEMAKER\n",
    "\n",
    "The Boto3 Container will be used to create the S3 buckets to store the preprocessed dataset.The Sagemaker's inbuilt XGBoost algorithm, will be built, trained and deployed.Including the use of optimal hyperparameters to get the best results for the RMSE( Root Mean Squared Error).An Endpoint will be created after the model is built.\n",
    "The Endpoint created awill be used to predict the price per metre square when the inputs of \"Quartier\" and \"Land size\" are fed to the endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II) SCRAPING THE DATA\n",
    "We will perform the following tasks, in order to successully scrape the data we need\n",
    "- a.) Importing the necessary Libraries \n",
    "- b.) Writing the ETL functions to obtain the data \n",
    "- c.) Scraping and storing the data to a dictionary\n",
    "- d.) Saving the final scraped dataframe to a CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a.) Importing all the necessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries required to scrape the data\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.) Writing ETL functions to Extract and Load the data to a Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the  function using Request and BeautifulSoup to get the URL of the pages we will need to scrape \n",
    "def get_urls(page_number):\n",
    "    base_url = 'https://www.jumia.cm'\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.54 Safari/537.36'}\n",
    "    request = requests.get(f'https://www.jumia.cm/en/land-plots?page={page_number}&xhr=ugmii', headers)\n",
    "    soup = BeautifulSoup(request.text, 'html.parser')\n",
    "    partial_url_list = soup.find_all('article')\n",
    "    for partial_url in partial_url_list:\n",
    "        new_url = base_url + partial_url.find('a')['href']\n",
    "        url_list.append(new_url)\n",
    "        print(f\"Getting the Urls for page {page_number}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function using BeautifulSoup to parse URLs from all the pages from the above function \n",
    "def extract_page(url):\n",
    "    url = url\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.54 Safari/537.36'}\n",
    "    request = requests.get(url, headers)\n",
    "    soup = BeautifulSoup(request.text, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to obtain the data we need from all those URLs above and store in a dictionary\n",
    "def transform_page(soup):\n",
    "    main_div = soup.find('div', class_='twocolumn')\n",
    "    price = main_div.find('span', {'class': 'price'}).get_text(strip=True).replace('FCFA',\"\")\n",
    "    location = main_div.select('dl > dd')[1].text.strip()\n",
    "    try:\n",
    "        area = main_div.find_all('h3')[1].get_text(strip=True).replace('Area', \"\").replace(' m2',\"\")\n",
    "    except IndexError:\n",
    "        area = ''\n",
    "\n",
    "    items = {\n",
    "        'Price': price,\n",
    "        'Location': location,\n",
    "        'Area': area\n",
    "    }\n",
    "    land_data_list.append(items)\n",
    "\n",
    "    print(f\"Scrapping the page '{soup.find('title').text}'...\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c.) Scraping and Storing the data into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the Urls for page 1\n",
      "Getting the Urls for page 1\n",
      "Getting the Urls for page 1\n",
      "Getting the Urls for page 1\n",
      "Getting the Urls for page 1\n",
      "Getting the Urls for page 1\n",
      "Getting the Urls for page 1\n",
      "Getting the Urls for page 1\n",
      "Getting the Urls for page 1\n"
     ]
    }
   ],
   "source": [
    "# Extracting all the URLs from page 1 to the number of pages required.In this case I just extracted 1 page as a demo\n",
    "url_list = []\n",
    "for page_number in range(1, 2):\n",
    "    get_urls(page_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapping the page 'Terrain Titré 2000 M² À Odza  | Odza | Jumia Deals'...\n",
      "Scrapping the page 'OPPORTUNITÉ De TERRAIN À OMNISPORT | Omnisports | Jumia Deals'...\n",
      "Scrapping the page 'Vente terrain titré de 2hectared à Bastos | Bastos | Jumia Deals'...\n",
      "Scrapping the page 'Terrain À Vendre a Bonaberi ( Bonadale) | Bonaberi | Jumia Deals'...\n",
      "Scrapping the page 'Terrain Titré Et Viable a Ngombé | Douala | Jumia Deals'...\n",
      "Scrapping the page 'A Vendre  Terrain | Limbé | Jumia Deals'...\n",
      "Scrapping the page 'A Vendre  Terrain | Limbé | Jumia Deals'...\n",
      "Scrapping the page 'A Vendre  Terrain | Limbé | Jumia Deals'...\n",
      "Scrapping the page 'Terrain À Vendre a Bonaberi ( BONADALE) | Bonaberi | Jumia Deals'...\n"
     ]
    }
   ],
   "source": [
    "#Extracting and Transfroming all the data from the required pages selected above\n",
    "land_data_list = []\n",
    "for url in url_list:\n",
    "    page = extract_page(url)\n",
    "    transform_page(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  #### d.) Saving the scraped data as a CSV file using pandas   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing first 05 elements...\n",
      "     Price    Location    Area\n",
      "0   15,500        Odza    2000\n",
      "1   65,000  Omnisports     630\n",
      "2  500,000      Bastos  20.000\n",
      "3    8,000    Bonaberi    5000\n",
      "4    7,000      Douala    8500\n"
     ]
    }
   ],
   "source": [
    "# Creating a pandas dataframe\n",
    "df = pd.DataFrame(land_data_list)\n",
    "print('Printing first 05 elements...')\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9 entries, 0 to 8\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Price     9 non-null      object\n",
      " 1   Location  9 non-null      object\n",
      " 2   Area      9 non-null      object\n",
      "dtypes: object(3)\n",
      "memory usage: 344.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formating Area and Price Columns from text to numeric\n",
    "df['Area'].replace({' m2':'',',': ''},regex = True,inplace = True)\n",
    "df['Area'] = pd.to_numeric(df['Area'],errors = 'coerce')\n",
    "\n",
    "df['Price'].replace({'FCFA':'',',': ''},regex = True,inplace = True)\n",
    "df['Price'] = pd.to_numeric(df['Price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9 entries, 0 to 8\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Price     9 non-null      int64  \n",
      " 1   Location  9 non-null      object \n",
      " 2   Area      9 non-null      float64\n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 344.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Location</th>\n",
       "      <th>Area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15500</td>\n",
       "      <td>Odza</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65000</td>\n",
       "      <td>Omnisports</td>\n",
       "      <td>630.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500000</td>\n",
       "      <td>Bastos</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8000</td>\n",
       "      <td>Bonaberi</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7000</td>\n",
       "      <td>Douala</td>\n",
       "      <td>8500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Price    Location    Area\n",
       "0   15500        Odza  2000.0\n",
       "1   65000  Omnisports   630.0\n",
       "2  500000      Bastos    20.0\n",
       "3    8000    Bonaberi  5000.0\n",
       "4    7000      Douala  8500.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_csv('land_price_data.csv',index = False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!!! We have finally scraped the data from the clasified adds website and saved as a csv (land_price_data.csv).Let us move on the the next phase of Exploratory Data Analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
