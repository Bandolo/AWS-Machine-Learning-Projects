{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAND PRICE PREDICTION APP USING AWS SAGEMAKER'S IN-BUILT XGBOOST  - End-to-End\n",
    "We will build a Land Price Prediction App to help people looking to buy land in Cameroon, get the expected price of land per quartier they intend to buy land from.\n",
    "The following steps will be taken:\n",
    "- I)   PROBLEM STATEMENT:\n",
    "\n",
    "Many people in Cameroon want to buy lands and they have trouble getting information on what to expect as price per square metre for the quartier they want to buy the land from.They also want to be able to consult the prices of several quartiers before making their final choice.\n",
    "This is a difficult process in Cameroon as it will mean these people who want to buy lands will have to go about making many phone calls to people asking them the price of land in those quartiers.\n",
    "So the objective is to scrape the data already available on the biggest Classified adds website in Cameroon (Jumia Cameroon) https://www.jumia.cm/en/land-plots\n",
    "\n",
    "This data will be cleaned and trained using the in-built XGBoost Algorithm on AWS Sagemaker, and an endpoint will be created in AWS ,which wll be used to make predictions when given the inputs like \n",
    "- The Quartier the customer wants to buy land from\n",
    "- The size of the land the customer intends to buy (in metres square)\n",
    "- And the output of the model will be the predicted Price per metres square for the Quartier the customer requested.\n",
    "\n",
    "\n",
    "- II)   SCRAPING THE DATA:\n",
    "\n",
    "Scrape the data from a Classified ads website, where people post lands for sale per quartier in Cameroon.They typically type in the price per metres square and the total area of the land availlable for sale.\n",
    "- III)  PERFORM EXPLORATORY DATA ANALYSIS \n",
    "\n",
    "Inspect the data to validate the quality of the data scraped from the classified ads website. Analyse the distribution of missing values, outliers and gain other relevant insights from the model\n",
    "- IV) DO FEATURE ENGINEERING & SELECTION\n",
    "\n",
    "Handle the mising values, outliers and do the necessary transformations which will ensure the data is well suited for the machine learning model.And also to maximise the insights gotten from the Exploratory Data Analysis phase.\n",
    "- V)  BUILD,TRAIN AND DEPLOY THE MODEL IN SAGEMAKER\n",
    "\n",
    "The Boto3 Container will be used to create the S3 buckets to store the preprocessed dataset.The Sagemaker's inbuilt XGBoost algorithm, will be built, trained and deployed.Including the use of optimal hyperparameters to get the best results for the RMSE( Root Mean Squared Error).An Endpoint will be created after the model is built.\n",
    "The Endpoint created awill be used to predict the price per metre square when the inputs of \"Quartier\" and \"Land size\" are fed to the endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V) BUILD,TRAIN AND DEPLOY THE MODEL IN SAGEMAKER\n",
    "We will perform the following tasks, in order to successully scrape the data we need\n",
    "- a.) Importing the necessary Libraries and create S3 bucket\n",
    "- b.) Download the train and test data and store in S3\n",
    "- c.) Build and Train the Inbuilt XGBoost model\n",
    "- d.) Deploy the model to an Endpoint\n",
    "- e.) Test the predictions\n",
    "- f.) Delete the Endpoint\n",
    "- g.) Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a.) Importing all the necessary libraries and creating S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri \n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker.session import s3_input, Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-east-1\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'landpriceapp' # <--- Give this a unique name, since there can be no 02 bucket names in AWS\n",
    "my_region = boto3.session.Session().region_name # set the region of the instance\n",
    "print(my_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 bucket created successfully\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "try:\n",
    "    if  my_region == 'us-east-1':\n",
    "        s3.create_bucket(Bucket=bucket_name)\n",
    "    print('S3 bucket created successfully')\n",
    "except Exception as e:\n",
    "    print('S3 error: ',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://landpriceapp/xgboost-inbuilt-algo/output\n"
     ]
    }
   ],
   "source": [
    "# set an output path where the trained model will be saved\n",
    "prefix = 'xgboost-inbuilt-algo'\n",
    "output_path ='s3://{}/{}/output'.format(bucket_name, prefix)\n",
    "print(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.) Download the train and test data and store in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import urllib\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None) #setting pandas to display all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: downloaded train_clean.csv.\n",
      "Success: Data loaded into dataframe.\n"
     ]
    }
   ],
   "source": [
    "#Importing the train dataset\n",
    "try:\n",
    "    urllib.request.urlretrieve (\"https://raw.githubusercontent.com/Bandolo/AWS-Machine-Learning-Projects/main/LandPriceApp-XGBoost-Sagemaker/train_clean.csv\", \"train_clean.csv\")\n",
    "    print('Success: downloaded train_clean.csv.')\n",
    "except Exception as e:\n",
    "    print('Data load error: ',e)\n",
    "\n",
    "try:\n",
    "    train_clean = pd.read_csv('./train_clean.csv')\n",
    "    print('Success: Data loaded into dataframe.')\n",
    "except Exception as e:\n",
    "    print('Data load error: ',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: downloaded train_clean.csv.\n",
      "Success: Data loaded into dataframe.\n"
     ]
    }
   ],
   "source": [
    "#Importing the test dataset\n",
    "try:\n",
    "    urllib.request.urlretrieve (\"https://raw.githubusercontent.com/Bandolo/AWS-Machine-Learning-Projects/main/LandPriceApp-XGBoost-Sagemaker/test_clean.csv\", \"test_clean.csv\")\n",
    "    print('Success: downloaded train_clean.csv.')\n",
    "except Exception as e:\n",
    "    print('Data load error: ',e)\n",
    "\n",
    "try:\n",
    "    test_clean = pd.read_csv('./test_clean.csv')\n",
    "    print('Success: Data loaded into dataframe.')\n",
    "except Exception as e:\n",
    "    print('Data load error: ',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(624, 26)\n"
     ]
    }
   ],
   "source": [
    "print(test_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Area  Price_log  Awae  Bastos  Bonaberi  Bonamoussadi  Japoma  Kotto  \\\n",
      "0  3000.0  10.463103     0       0         0             0       0      0   \n",
      "1   200.0  11.608236     0       0         0             0       0      0   \n",
      "2   450.0  10.596635     0       0         0             0       0      0   \n",
      "3   798.0  10.596635     0       0         0             0       0      0   \n",
      "4  1900.0   9.798127     0       0         0             0       0      0   \n",
      "\n",
      "   Kribi  Lendi  Limbé  Logbessou  Logpom  Makepe  Mfou  Nkoabang  Odza  \\\n",
      "0      1      0      0          0       0       0     0         0     0   \n",
      "1      0      0      0          0       0       1     0         0     0   \n",
      "2      0      0      0          0       0       0     0         0     0   \n",
      "3      0      0      0          0       0       0     0         0     1   \n",
      "4      0      0      0          0       0       0     0         0     0   \n",
      "\n",
      "   Olembe  Omnisports  PK12  PK14  PK16  Soa  Village  Yaoundé  Yassa  \n",
      "0       0           0     0     0     0    0        0        0      0  \n",
      "1       0           0     0     0     0    0        0        0      0  \n",
      "2       0           0     0     0     0    0        0        0      1  \n",
      "3       0           0     0     0     0    0        0        0      0  \n",
      "4       0           0     0     0     0    0        0        0      1  \n"
     ]
    }
   ],
   "source": [
    "print(train_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Area  Price_log  Awae  Bastos  Bonaberi  Bonamoussadi  Japoma  Kotto  \\\n",
      "0   500.0  10.463103     0       0         0             0       0      0   \n",
      "1   500.0  11.002100     0       0         0             0       0      1   \n",
      "2  3000.0   7.949091     0       0         0             0       0      0   \n",
      "3   325.0  11.512925     0       0         0             0       0      0   \n",
      "4  1000.0   9.903488     0       0         0             0       0      0   \n",
      "\n",
      "   Kribi  Lendi  Limbé  Logbessou  Logpom  Makepe  Mfou  Nkoabang  Odza  \\\n",
      "0      0      0      0          0       0       0     0         0     0   \n",
      "1      0      0      0          0       0       0     0         0     0   \n",
      "2      0      0      0          0       0       0     1         0     0   \n",
      "3      0      0      0          0       0       1     0         0     0   \n",
      "4      0      0      0          0       0       0     0         0     1   \n",
      "\n",
      "   Olembe  Omnisports  PK12  PK14  PK16  Soa  Village  Yaoundé  Yassa  \n",
      "0       0           0     0     0     0    0        0        1      0  \n",
      "1       0           0     0     0     0    0        0        0      0  \n",
      "2       0           0     0     0     0    0        0        0      0  \n",
      "3       0           0     0     0     0    0        0        0      0  \n",
      "4       0           0     0     0     0    0        0        0      0  \n"
     ]
    }
   ],
   "source": [
    "print(test_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving Train And Test Into Buckets\n",
    "## We start with Train Data\n",
    "import os\n",
    "pd.concat([train_clean['Price_log'], train_clean.drop(['Price_log'], \n",
    "                                                axis=1)], \n",
    "                                                axis=1).to_csv('train.csv', index=False, header=False)\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "s3_input_train = sagemaker.TrainingInput(s3_data='s3://{}/{}/train'.format(bucket_name, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data Into Buckets\n",
    "pd.concat([test_clean['Price_log'], test_clean.drop(['Price_log'], \n",
    "                                              axis=1)], axis=1).to_csv('test.csv', index=False, header=False)\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'test/test.csv')).upload_file('test.csv')\n",
    "s3_input_test = sagemaker.TrainingInput(s3_data='s3://{}/{}/test'.format(bucket_name, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c.) Build and Train the Inbuilt XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\n",
    "# specify the repo_version depending on your preference.\n",
    "container = retrieve('xgboost',boto3.Session().region_name,'latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize hyperparameters\n",
    "hyperparameters = {\n",
    "        \"max_depth\":\"5\",\n",
    "        \"eta\":\"0.25\",\n",
    "        \"gamma\":\"0.3\",\n",
    "        \"min_child_weight\":\"7\",\n",
    "        \"subsample\":\"1\",\n",
    "        \"objective\":\"reg:linear\",\n",
    "        \"num_round\":50\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a SageMaker estimator that calls the xgboost-container\n",
    "estimator = sagemaker.estimator.Estimator(image_uri=container, \n",
    "                                          hyperparameters=hyperparameters,\n",
    "                                          role=sagemaker.get_execution_role(),\n",
    "                                          instance_count=1, \n",
    "                                          instance_type='ml.m5.2xlarge', \n",
    "                                          volume_size=5, # 5 GB \n",
    "                                          output_path=output_path,\n",
    "                                          use_spot_instances=True,\n",
    "                                          max_run=300,\n",
    "                                          max_wait=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-25 15:06:07 Starting - Starting the training job...\n",
      "2022-02-25 15:06:16 Starting - Launching requested ML instancesProfilerReport-1645801567: InProgress\n",
      ".........\n",
      "2022-02-25 15:07:57 Starting - Preparing the instances for training......\n",
      "2022-02-25 15:09:08 Downloading - Downloading input data...\n",
      "2022-02-25 15:09:38 Training - Training image download completed. Training in progress.\n",
      "2022-02-25 15:09:38 Uploading - Uploading generated training model\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[2022-02-25:15:09:30:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[34m[2022-02-25:15:09:30:INFO] File size need to be processed in the node: 0.22mb. Available memory size in the node: 23769.68mb\u001b[0m\n",
      "\u001b[34m[2022-02-25:15:09:30:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[15:09:30] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[15:09:30] 2495x25 matrix with 62375 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[2022-02-25:15:09:30:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[15:09:30] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[15:09:30] 624x25 matrix with 15600 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2\u001b[0m\n",
      "\u001b[34m[0]#011train-rmse:7.51516#011validation-rmse:7.59877\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[1]#011train-rmse:5.68954#011validation-rmse:5.76459\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[2]#011train-rmse:4.33138#011validation-rmse:4.39717\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[3]#011train-rmse:3.32955#011validation-rmse:3.3895\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[4]#011train-rmse:2.59395#011validation-rmse:2.64746\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[5]#011train-rmse:2.06782#011validation-rmse:2.11864\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[6]#011train-rmse:1.6981#011validation-rmse:1.74898\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[7]#011train-rmse:1.44708#011validation-rmse:1.49536\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[8]#011train-rmse:1.28102#011validation-rmse:1.32625\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[9]#011train-rmse:1.17599#011validation-rmse:1.21309\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10]#011train-rmse:1.11077#011validation-rmse:1.14497\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[11]#011train-rmse:1.07005#011validation-rmse:1.10233\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[12]#011train-rmse:1.043#011validation-rmse:1.07355\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[13]#011train-rmse:1.01596#011validation-rmse:1.04744\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[14]#011train-rmse:1.00588#011validation-rmse:1.03577\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[15]#011train-rmse:0.998499#011validation-rmse:1.0303\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[16]#011train-rmse:0.993389#011validation-rmse:1.02577\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17]#011train-rmse:0.986073#011validation-rmse:1.0161\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[18]#011train-rmse:0.983384#011validation-rmse:1.01466\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[19]#011train-rmse:0.97844#011validation-rmse:1.00815\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[20]#011train-rmse:0.976852#011validation-rmse:1.00596\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[21]#011train-rmse:0.975064#011validation-rmse:1.00518\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22]#011train-rmse:0.97324#011validation-rmse:1.00415\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[23]#011train-rmse:0.970316#011validation-rmse:1.00013\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[24]#011train-rmse:0.969237#011validation-rmse:0.999006\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[25]#011train-rmse:0.96664#011validation-rmse:0.99537\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[26]#011train-rmse:0.965337#011validation-rmse:0.99554\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[27]#011train-rmse:0.962117#011validation-rmse:0.992718\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[28]#011train-rmse:0.95784#011validation-rmse:0.991876\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[29]#011train-rmse:0.956464#011validation-rmse:0.990758\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[30]#011train-rmse:0.954963#011validation-rmse:0.991006\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[31]#011train-rmse:0.952105#011validation-rmse:0.989591\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[32]#011train-rmse:0.951226#011validation-rmse:0.989471\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[33]#011train-rmse:0.949283#011validation-rmse:0.985899\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[34]#011train-rmse:0.948566#011validation-rmse:0.984404\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[35]#011train-rmse:0.946314#011validation-rmse:0.98408\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[36]#011train-rmse:0.945943#011validation-rmse:0.983576\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[37]#011train-rmse:0.941924#011validation-rmse:0.982006\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[38]#011train-rmse:0.940814#011validation-rmse:0.980284\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[39]#011train-rmse:0.939226#011validation-rmse:0.980527\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[40]#011train-rmse:0.938421#011validation-rmse:0.980084\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[41]#011train-rmse:0.937157#011validation-rmse:0.979311\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[42]#011train-rmse:0.93584#011validation-rmse:0.977473\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[43]#011train-rmse:0.934855#011validation-rmse:0.976503\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[44]#011train-rmse:0.933941#011validation-rmse:0.974634\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[45]#011train-rmse:0.933024#011validation-rmse:0.97559\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[46]#011train-rmse:0.932609#011validation-rmse:0.975017\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[47]#011train-rmse:0.93144#011validation-rmse:0.97357\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[48]#011train-rmse:0.931017#011validation-rmse:0.973775\u001b[0m\n",
      "\u001b[34m[15:09:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[49]#011train-rmse:0.930285#011validation-rmse:0.972854\u001b[0m\n",
      "\n",
      "2022-02-25 15:09:58 Completed - Training job completed\n",
      "Training seconds: 36\n",
      "Billable seconds: 15\n",
      "Managed Spot Training savings: 58.3%\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'train': s3_input_train,'validation': s3_input_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d.) Deploy the model to an Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------!"
     ]
    }
   ],
   "source": [
    "xgb_predictor = estimator.deploy(initial_instance_count=1,instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e.) Test the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(624,)\n"
     ]
    }
   ],
   "source": [
    "#from sagemaker.predictor import csv_serializer\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "test_data_array = test_clean.drop(['Price_log'], axis=1).values #load the data into an array\n",
    "#xgb_predictor.content_type = 'text/csv' # set the data type for an inference\n",
    "xgb_predictor.serializer = CSVSerializer() # set the serializer type\n",
    "predictions = xgb_predictor.predict(test_data_array).decode('utf-8') # predict!\n",
    "predictions_array = np.fromstring(predictions[1:], sep=',') # and turn the prediction into an array\n",
    "print(predictions_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.99798775, 10.81545544,  8.42318058, 11.17301559, 10.12899113,\n",
       "       10.35171318, 10.78121758, 10.18690586, 10.59714508, 10.59128189])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_array[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>Price_log</th>\n",
       "      <th>Awae</th>\n",
       "      <th>Bastos</th>\n",
       "      <th>Bonaberi</th>\n",
       "      <th>Bonamoussadi</th>\n",
       "      <th>Japoma</th>\n",
       "      <th>Kotto</th>\n",
       "      <th>Kribi</th>\n",
       "      <th>Lendi</th>\n",
       "      <th>...</th>\n",
       "      <th>Odza</th>\n",
       "      <th>Olembe</th>\n",
       "      <th>Omnisports</th>\n",
       "      <th>PK12</th>\n",
       "      <th>PK14</th>\n",
       "      <th>PK16</th>\n",
       "      <th>Soa</th>\n",
       "      <th>Village</th>\n",
       "      <th>Yaoundé</th>\n",
       "      <th>Yassa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500.0</td>\n",
       "      <td>10.463103</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500.0</td>\n",
       "      <td>11.002100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>7.949091</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>325.0</td>\n",
       "      <td>11.512925</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>9.903488</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>10.463103</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2900.0</td>\n",
       "      <td>11.512925</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>10.463103</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>482.0</td>\n",
       "      <td>10.463103</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>655.0</td>\n",
       "      <td>11.156251</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Area  Price_log  Awae  Bastos  Bonaberi  Bonamoussadi  Japoma  Kotto  \\\n",
       "0   500.0  10.463103     0       0         0             0       0      0   \n",
       "1   500.0  11.002100     0       0         0             0       0      1   \n",
       "2  3000.0   7.949091     0       0         0             0       0      0   \n",
       "3   325.0  11.512925     0       0         0             0       0      0   \n",
       "4  1000.0   9.903488     0       0         0             0       0      0   \n",
       "5  1000.0  10.463103     0       0         0             0       0      0   \n",
       "6  2900.0  11.512925     0       0         0             0       0      0   \n",
       "7  1000.0  10.463103     0       0         0             0       0      0   \n",
       "8   482.0  10.463103     0       0         0             0       0      0   \n",
       "9   655.0  11.156251     0       0         0             0       0      0   \n",
       "\n",
       "   Kribi  Lendi  ...  Odza  Olembe  Omnisports  PK12  PK14  PK16  Soa  \\\n",
       "0      0      0  ...     0       0           0     0     0     0    0   \n",
       "1      0      0  ...     0       0           0     0     0     0    0   \n",
       "2      0      0  ...     0       0           0     0     0     0    0   \n",
       "3      0      0  ...     0       0           0     0     0     0    0   \n",
       "4      0      0  ...     1       0           0     0     0     0    0   \n",
       "5      0      0  ...     0       0           0     0     0     0    0   \n",
       "6      0      0  ...     0       0           0     0     0     0    0   \n",
       "7      0      0  ...     0       0           0     0     0     0    0   \n",
       "8      0      0  ...     0       0           0     0     0     0    0   \n",
       "9      1      0  ...     0       0           0     0     0     0    0   \n",
       "\n",
       "   Village  Yaoundé  Yassa  \n",
       "0        0        1      0  \n",
       "1        0        0      0  \n",
       "2        0        0      0  \n",
       "3        0        0      0  \n",
       "4        0        0      0  \n",
       "5        0        0      1  \n",
       "6        0        0      0  \n",
       "7        0        0      0  \n",
       "8        0        0      0  \n",
       "9        0        0      0  \n",
       "\n",
       "[10 rows x 26 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_clean.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Predicted Price\n",
      "0              3.0\n",
      "1          49784.0\n",
      "2           4551.0\n",
      "3          71183.0\n",
      "4          25059.0\n",
      "5          31311.0\n",
      "6          48109.0\n",
      "7          26553.0\n",
      "8          40020.0\n",
      "9          39786.0\n"
     ]
    }
   ],
   "source": [
    "prediction = pd.DataFrame(np.exp(predictions_array[0:10]),columns=[\"Predicted Price\"])\n",
    "print(round(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     35000.0\n",
      "1     60000.0\n",
      "2      2833.0\n",
      "3    100000.0\n",
      "4     20000.0\n",
      "5     35000.0\n",
      "6    100000.0\n",
      "7     35000.0\n",
      "8     35000.0\n",
      "9     70000.0\n",
      "Name: Price_log, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "actual = np.exp(test_clean.Price_log.head(10))\n",
    "print(actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f.) Deleting The Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sagemaker' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-ecd069109aa0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msagemaker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete_endpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxgb_predictor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mbucket_to_delete\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mboto3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresource\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m's3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBucket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbucket_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mbucket_to_delete\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjects\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sagemaker' is not defined"
     ]
    }
   ],
   "source": [
    "sagemaker.Session().delete_endpoint(xgb_predictor.endpoint)\n",
    "bucket_to_delete = boto3.resource('s3').Bucket(bucket_name)\n",
    "bucket_to_delete.objects.all().delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations!!! You just built an end-to-end machine learning app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### g.)Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whe have successfully gone through the machine learning cycle from Problem framing till deployment.And since Machine Learning is an iterative process, we can always go back and optimise from Feature Engineering to improve our accuracy.\n",
    "\n",
    "The __accuracy__ of the current model is low, due to the following reasons:\n",
    "- The __prices per m2__ square was poorly entered by some sales agents on the Jumia website. Instead of entering price per metres squared, they entered the total price of the whole piece of land.\n",
    "- There are still __some farmlands__ in the causing variations in prices from the price for redential areas.\n",
    "- The __house prices vary alot__, so the model is finding it hard to predict a price which is close to the actual price we are measutrig at the time of model evaluation.\n",
    "\n",
    "Possible __corrective actions__ can be taken to improve the efficiency of the model:\n",
    "- Recruit junior staff to go through the prices and __correct the wrong prices__ entered by the sales agents\n",
    "- Complety eliminate observations with very low prices, which could indicate __farmlands__\n",
    "- Or simply use the __median house price per location__, irrespective of the metres square bought.\n",
    "\n",
    "Feel free to use any resources here to improve on the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wish you Good Data Luck!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
