{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAND PRICE PREDICTION APP USING AWS SAGEMAKER - End-to-End\n",
    "We will build a simple Land Price Prediction App to help people looking to buy land in Cameroon, get the expected price per quartier they intend to buy land from.\n",
    "As seen in the Best Practices for Machine Learning Projects on AWS, the following steps will be taken:\n",
    "- I)   SCRAPING THE DATA\n",
    "- II)  IMPORTING THE DATA INTO SAGEMAKER \n",
    "- III) EXPLORATORY DATA ANALYSIS IN SAGEMAKER\n",
    "- IV)  FEATURE ENGINEERING IN SAGEMAKER\n",
    "- V)   PREDICTIVE MODEL BUILDING AND DEPLOYMENT IN SAGEMAKER\n",
    "- VI)  MODEL INFERERENCE IN SAGEMAKER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I) SCRAPING THE DATA\n",
    "We will perform the following tasks in order to successully scrape the data we need\n",
    "- a.) Importing the necessary Libraries \n",
    "- b.) Writing the ETL functions to obtain the data \n",
    "- c.) Scraping and storing the data to a dictionary\n",
    "- d.)Saving the final scraped dataframe to a CSV file using pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a.) Importing the necessary Libraries to scrape the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.) Writing ETL functions to Extract and Load the data to a Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function using Request and BeautifulSoup to get the URL of the pages we will need to scrape \n",
    "def get_urls(page_number):\n",
    "    base_url = 'https://www.jumia.cm'\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.54 Safari/537.36'}\n",
    "    request = requests.get(f'https://www.jumia.cm/en/land-plots?page={page_number}&xhr=ugmii', headers)\n",
    "    soup = BeautifulSoup(request.text, 'html.parser')\n",
    "    partial_url_list = soup.find_all('article')\n",
    "    for partial_url in partial_url_list:\n",
    "        new_url = base_url + partial_url.find('a')['href']\n",
    "        url_list.append(new_url)\n",
    "        print(f\"Getting the Urls for page {page_number}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function using BeautifulSoup to parse URLs from all the pages from the above function \n",
    "def extract_page(url):\n",
    "    url = url\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.54 Safari/537.36'}\n",
    "    request = requests.get(url, headers)\n",
    "    soup = BeautifulSoup(request.text, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to obtain the data we need from all those URLs above and store in a dictionary\n",
    "def transform_page(soup):\n",
    "    main_div = soup.find('div', class_='twocolumn')\n",
    "    price = main_div.find('span', {'class': 'price'}).get_text(strip=True)\n",
    "    location = main_div.select('dl > dd')[1].text.strip()\n",
    "    try:\n",
    "        area = main_div.find_all('h3')[1].get_text(strip=True).replace('Area', \"\")\n",
    "    except IndexError:\n",
    "        area = ''\n",
    "\n",
    "    items = {\n",
    "        'Price': price,\n",
    "        'Location': location,\n",
    "        'Area': area\n",
    "    }\n",
    "    land_data_list.append(items)\n",
    "\n",
    "    print(f\"Scrapping the page '{soup.find('title').text}'...\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c.) Scraping and Storing the data into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the Urls for page 1\n",
      "Getting the Urls for page 1\n",
      "Getting the Urls for page 1\n",
      "Getting the Urls for page 1\n",
      "Getting the Urls for page 1\n",
      "Getting the Urls for page 1\n",
      "Getting the Urls for page 1\n",
      "Getting the Urls for page 1\n",
      "Getting the Urls for page 1\n"
     ]
    }
   ],
   "source": [
    "# Extracting all the URLs for from page 1 to the number of pages required\n",
    "url_list = []\n",
    "for page_number in range(1, 2):\n",
    "    get_urls(page_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapping the page 'Terrain  Titré à vendre Logbessou 200m2/1000m2 /500m2  | Douala | Jumia Deals'...\n",
      "Scrapping the page 'Terrain Titré en Vente | Mfou | Jumia Deals'...\n",
      "Scrapping the page 'Terrain  De 200 m² À Vendre | Mbalgong | Jumia Deals'...\n",
      "Scrapping the page 'terrain titré à vendre à l'échangeur Ahala | Ahala | Jumia Deals'...\n",
      "Scrapping the page 'Terrain Titré À Vendre À Nkolbisson (béatitude) | Nkolbisson | Jumia Deals'...\n",
      "Scrapping the page 'Terrain à vendre  | Emana | Jumia Deals'...\n",
      "Scrapping the page 'Terrain résidentiel à la cité chirac  | Yassa | Jumia Deals'...\n",
      "Scrapping the page 'Terrain titré à vendre | Olembe | Jumia Deals'...\n",
      "Scrapping the page 'Terrain De 500m² À Vendre | Bonaberi | Jumia Deals'...\n"
     ]
    }
   ],
   "source": [
    "#Extracting and Transfroming all the data from the required pages selected above\n",
    "land_data_list = []\n",
    "for url in url_list:\n",
    "    page = extract_page(url)\n",
    "    transform_page(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  #### d.) Saving the scraped data as a CSV file using pandas   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing first 05 elements...\n",
      "            Price    Location      Area\n",
      "0      60,000FCFA      Douala   1000 m2\n",
      "1         700FCFA        Mfou  30000 m2\n",
      "2   1,500,000FCFA    Mbalgong    200 m2\n",
      "3  10,000,003FCFA       Ahala    300 m2\n",
      "4      10,000FCFA  Nkolbisson    500 m2\n"
     ]
    }
   ],
   "source": [
    "# Exporting to CSV\n",
    "df = pd.DataFrame(land_data_list)\n",
    "print('Printing first 05 elements...')\n",
    "print(df.head())\n",
    "df.to_csv('land_price_data.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
