{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAND PRICE PREDICTION APP USING AWS SAGEMAKER - End-to-End\n",
    "We will build a Land Price Prediction App to help people looking to buy land in Cameroon.This app will help them get the expected price per quartier,when they enter the name of the neighbourhood and the size of landthey intend to purchase in that neighbourhood.The app will return the price per metre square of land in that neighbourhood.They can also enter other neighbourhoods to compare the prices.\n",
    "As seen in the Best Practices for Machine Learning Projects on AWS, and also by the CRISP-DM process, the following steps will be taken to build this machin learning app:\n",
    "- I)   PROBLEM STATEMENT:\n",
    "\n",
    "Many people in Cameroon want to buy lands and they have trouble getting informatoon of what to expect as price per suare metre for the quartier they want to buy the land in.They also want to be able to consult the prices of several quartiers before making their choice.\n",
    "This is a difficult process in Cameroon as it will mean these people who want to buy lands will have to go about making phone calls and asking pople what are the prices of lands in different areas.\n",
    "So the objective is to scrape the data already available on the biggest Classified adds website in Cameroon (Jumia Cameroon) https://www.jumia.cm/en/land-plots\n",
    "\n",
    "This data will be cleaned and trained using the in-built XGBoost Algorithm on AWS Sagemaker, and an endpoint will be created in AWS ,which wll be used to make predictions when given the inputs like \n",
    "- The Quartier the customer wants to buy land from\n",
    "- The size of the land the customer intends to buy (in metres square)\n",
    "- And the outputt of the model will be the predicted Price per metre square for the Quartier the customer requested.\n",
    "\n",
    "\n",
    "- II)   SCRAPING THE DATA:\n",
    "\n",
    "Scrape the data from a Classified ads website, where people post lands for sale per quartier in Cameroon.They typically type in the price per metres square and the total area of the land availlable for sale\n",
    "- III)  PERFORM EXPLORATORY DATA ANALYSIS \n",
    "\n",
    "Inspect the data to validate the quality of the data scraped from the classified ads website.See the distribution of missing values, outliers and gain other insight which will be used in the Feature Engineering stage to better prepare the features for the machine learning model to be able to make accurate predictions.\n",
    "- IV) DO FEATURE ENGINEERING & SELECTION\n",
    "\n",
    "Handle the mising values, outliers and do the necessary transformations which will ensure the data is well suited for the machine learning model.And also to maximise the insights gotten from the Exploratory Data Analysis phase.\n",
    "- V)  BUILD,TRAIN AND DEPLOY THE MODEL IN SAGEMAKER\n",
    "\n",
    "The Boto3 Container will be used to create the S3 buckets to store the preprocessed dataset.The Sagemaker's inbuilt XGBoost algorithm, will be built, trained and deployed.Including the use of optimal hyperparameters to get the best results for the RMSE( Root Mean Squared Error).An Endpoint will be created after the model is built.\n",
    "\n",
    "- VI)   MODEL INFERENCE IN SAGEMAKER\n",
    "\n",
    "The Endpoint created above will be used to predict the price per metre square when the inputs of \"Quartier\" and \"Land size\" are entered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II) SCRAPING THE DATA\n",
    "We will perform the following tasks in order to successully scrape the data we need\n",
    "- a.) Importing all the necessary Libraries \n",
    "- b.) Writing the ETL functions to obtain the data \n",
    "- c.) Scraping and storing the data to a dictionary\n",
    "- d.)Saving the final scraped dataframe to a CSV file using pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a.) Importing all the necessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries required to scrape the data\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "#Importing Libraries required to store the scraped data in an AWS S3 bucket\n",
    "#import sagemaker\n",
    "#import boto3\n",
    "#from sagemaker.session import s3_input, Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.) Writing ETL functions to Extract and Load the data to a Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the  function using Request and BeautifulSoup to get the URL of the pages we will need to scrape \n",
    "def get_urls(page_number):\n",
    "    base_url = 'https://www.jumia.cm'\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.54 Safari/537.36'}\n",
    "    request = requests.get(f'https://www.jumia.cm/en/land-plots?page={page_number}&xhr=ugmii', headers)\n",
    "    soup = BeautifulSoup(request.text, 'html.parser')\n",
    "    partial_url_list = soup.find_all('article')\n",
    "    for partial_url in partial_url_list:\n",
    "        new_url = base_url + partial_url.find('a')['href']\n",
    "        url_list.append(new_url)\n",
    "        print(f\"Getting the Urls for page {page_number}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function using BeautifulSoup to parse URLs from all the pages from the above function \n",
    "def extract_page(url):\n",
    "    url = url\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.54 Safari/537.36'}\n",
    "    request = requests.get(url, headers)\n",
    "    soup = BeautifulSoup(request.text, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to obtain the data we need from all those URLs above and store in a dictionary\n",
    "def transform_page(soup):\n",
    "    main_div = soup.find('div', class_='twocolumn')\n",
    "    price = main_div.find('span', {'class': 'price'}).get_text(strip=True).replace('FCFA',\"\")\n",
    "    location = main_div.select('dl > dd')[1].text.strip()\n",
    "    try:\n",
    "        area = main_div.find_all('h3')[1].get_text(strip=True).replace('Area', \"\").replace(' m2',\"\")\n",
    "    except IndexError:\n",
    "        area = ''\n",
    "\n",
    "    items = {\n",
    "        'Price': price,\n",
    "        'Location': location,\n",
    "        'Area': area\n",
    "    }\n",
    "    land_data_list.append(items)\n",
    "\n",
    "    print(f\"Scrapping the page '{soup.find('title').text}'...\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c.) Scraping and Storing the data into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the Urls for page 1\n",
      "Getting the Urls for page 1\n",
      "Getting the Urls for page 1\n",
      "Getting the Urls for page 1\n",
      "Getting the Urls for page 1\n",
      "Getting the Urls for page 1\n",
      "Getting the Urls for page 1\n",
      "Getting the Urls for page 1\n",
      "Getting the Urls for page 1\n",
      "Getting the Urls for page 1\n"
     ]
    }
   ],
   "source": [
    "# Extracting all the URLs for from page 1 to the number of pages required\n",
    "url_list = []\n",
    "for page_number in range(1, 2):\n",
    "    get_urls(page_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapping the page 'Terrain 60 Hectares À Nkoabang À Vendre | Mfou | Jumia Deals'...\n",
      "Scrapping the page 'Terrain  Titré à vendre Logbessou 200m2/1000m2 /500m2  | Douala | Jumia Deals'...\n",
      "Scrapping the page 'TERRAIN A VENDRE A PK 13 | PK13 | Jumia Deals'...\n",
      "Scrapping the page 'Terrain titré à vendre a pk 19 | PK19 | Jumia Deals'...\n",
      "Scrapping the page 'TERRAIN TITRE A VENDRE A PK 12 | PK12 | Jumia Deals'...\n",
      "Scrapping the page 'TERRAIN TITRE A VENDRE A  ARIE ( après bocom village) | Village | Jumia Deals'...\n",
      "Scrapping the page 'Terrain à vendre  | Bonaberi | Jumia Deals'...\n",
      "Scrapping the page 'Terrain Titre a Vendre  a Pk 26 | PK26 | Jumia Deals'...\n",
      "Scrapping the page 'Terrain commercial à vendre : Odza 3000 m² | Odza | Jumia Deals'...\n",
      "Scrapping the page 'Terrain à vendre : Yaoundé - Simbock 1 200 m² | Yaoundé | Jumia Deals'...\n"
     ]
    }
   ],
   "source": [
    "#Extracting and Transfroming all the data from the required pages selected above\n",
    "land_data_list = []\n",
    "for url in url_list:\n",
    "    page = extract_page(url)\n",
    "    transform_page(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  #### d.) Saving the scraped data as a CSV file using pandas   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing first 05 elements...\n",
      "    Price Location  Area\n",
      "0     350     Mfou  6000\n",
      "1  60,000   Douala  1000\n",
      "2  35,000     PK13   387\n",
      "3  23,000     PK19   500\n",
      "4  30,000     PK12      \n"
     ]
    }
   ],
   "source": [
    "# Creating a pandas dataframe\n",
    "df = pd.DataFrame(land_data_list)\n",
    "print('Printing first 05 elements...')\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Price     10 non-null     object\n",
      " 1   Location  10 non-null     object\n",
      " 2   Area      10 non-null     object\n",
      "dtypes: object(3)\n",
      "memory usage: 368.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formating Area and Price Columns from text to numeric\n",
    "df['Area'].replace({' m2':'',',': ''},regex = True,inplace = True)\n",
    "df['Area'] = pd.to_numeric(df['Area'],errors = 'coerce')\n",
    "\n",
    "df['Price'].replace({'FCFA':'',',': ''},regex = True,inplace = True)\n",
    "df['Price'] = pd.to_numeric(df['Price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Price     10 non-null     int64  \n",
      " 1   Location  10 non-null     object \n",
      " 2   Area      9 non-null      float64\n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 368.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Location</th>\n",
       "      <th>Area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>350</td>\n",
       "      <td>Mfou</td>\n",
       "      <td>6000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60000</td>\n",
       "      <td>Douala</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35000</td>\n",
       "      <td>PK13</td>\n",
       "      <td>387.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23000</td>\n",
       "      <td>PK19</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30000</td>\n",
       "      <td>PK12</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Price Location    Area\n",
       "0    350     Mfou  6000.0\n",
       "1  60000   Douala  1000.0\n",
       "2  35000     PK13   387.0\n",
       "3  23000     PK19   500.0\n",
       "4  30000     PK12     NaN"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_csv('land_price_data.csv',index = False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
